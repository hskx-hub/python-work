import requests
from bs4 import BeautifulSoup
import pandas as pd
import matplotlib.pyplot as plt
from wordcloud import WordCloud
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import PolynomialFeatures
import numpy as np
import re
from collections import Counter
import os

import matplotlib
matplotlib.rcParams['font.sans-serif'] = ['SimHei']
matplotlib.rcParams['axes.unicode_minus'] = False

def clean_text(text):
    return re.sub(r'[^a-zA-Z ]', '', text.lower())

# å„ä¼šè®®ä¸å¯¹åº”URLæ¨¡æ¿
conference_config = {
    'neurips': {
        'years': list(range(2020, 2025)),
        'url_tpl': "https://dblp.org/db/conf/nips/neurips{}.html",
        'predict_years': [2025, 2026]
    },
    'aaai': {
        'years': list(range(2020, 2026)),
        'url_tpl': "https://dblp.org/db/conf/aaai/aaai{}.html",
        'predict_years': [2026]
    },
    'kdd': {
        'years': list(range(2020, 2025)),
        'url_tpl': "https://dblp.org/db/conf/kdd/kdd{}.html",
        'predict_years': [2025, 2026]
    }
}

def fetch_papers(conf_name, url_tpl, years):
    all_papers = []
    valid_years = []
    for year in years:
        url = url_tpl.format(year)
        print(f"ğŸ“¥ Fetching {conf_name.upper()} {year} from {url}")
        res = requests.get(url)
        if res.status_code != 200:
            print(f"âš ï¸ è¯·æ±‚å¤±è´¥ - HTTP {res.status_code}: {url}")
            continue
        soup = BeautifulSoup(res.text, 'html.parser')
        papers = soup.find_all('cite', class_='data')
        for p in papers:
            title_tag = p.find('span', class_='title')
            if not title_tag:
                continue
            title = title_tag.text.strip()
            authors = [a.text for a in p.find_all('span', itemprop='author')]
            link_tag = p.find('a', href=True)
            link = link_tag['href'] if link_tag else ''
            all_papers.append({
                'title': title,
                'authors': ', '.join(authors),
                'year': year,
                'conference': f"{conf_name.upper()} {year}",
                'conf': conf_name,
                'url': link
            })
        valid_years.append(year)
    return all_papers, valid_years

csv_file = "all_conference_papers_2020_2024.csv"

if os.path.exists(csv_file):
    print(f"âœ… æ–‡ä»¶ {csv_file} å­˜åœ¨ï¼Œè¯»å–æ•°æ®ï¼Œè·³è¿‡çˆ¬å–")
    df = pd.read_csv(csv_file, encoding='utf-8-sig')
else:
    print(f"ğŸ“¥ æ–‡ä»¶ {csv_file} ä¸å­˜åœ¨ï¼Œå¼€å§‹çˆ¬å–æ•°æ®")
    all_data = []
    valid_years_map = {}

    for conf, cfg in conference_config.items():
        data, valid_years = fetch_papers(conf, cfg['url_tpl'], cfg['years'])
        all_data.extend(data)
        valid_years_map[conf] = valid_years

    df = pd.DataFrame(all_data)
    if df.empty:
        print("âŒ æœªè·å–ä»»ä½•è®ºæ–‡æ•°æ®")
        exit()

    df.to_csv(csv_file, index=False, encoding='utf-8-sig')

# é€ä¼šè®®é¢„æµ‹ä¸å¯è§†åŒ–
for conf, cfg in conference_config.items():
    sub_df = df[df['conf'] == conf].copy()
    if sub_df.empty:
        continue
    sub_df['year'] = sub_df['year'].astype(int)
    counts = sub_df.groupby('year').size()
    X = np.array(counts.index).reshape(-1, 1)
    y = counts.values

    # åˆ¤æ–­æ˜¯å¦æ˜¯AAAIï¼Œä½¿ç”¨å¤šé¡¹å¼å›å½’
    if conf == 'aaai':
        degree = 2
        poly = PolynomialFeatures(degree)
        X_poly = poly.fit_transform(X)
        model = LinearRegression().fit(X_poly, y)

        # é¢„æµ‹æœªæ¥å¹´ä»½
        X_pred_years = np.array(cfg['predict_years']).reshape(-1, 1)
        X_pred_poly = poly.transform(X_pred_years)
        preds = model.predict(X_pred_poly)

        # æ›²çº¿æ‹Ÿåˆæ•°æ®
        X_curve = np.linspace(X.min(), max(cfg['predict_years']), 300).reshape(-1, 1)
        X_curve_poly = poly.transform(X_curve)
        y_curve = model.predict(X_curve_poly)
    else:
        # ä½¿ç”¨çº¿æ€§å›å½’
        model = LinearRegression().fit(X, y)
        X_pred_years = np.array(cfg['predict_years']).reshape(-1, 1)
        preds = model.predict(X_pred_years)

        # æ›²çº¿æ‹Ÿåˆæ•°æ®
        X_curve = np.linspace(X.min(), max(cfg['predict_years']), 300).reshape(-1, 1)
        y_curve = model.predict(X_curve)

    # è¾“å‡ºé¢„æµ‹ç»“æœ
    for year, pred in zip(cfg['predict_years'], preds):
        print(f"ğŸ”® é¢„æµ‹ {conf.upper()} {year} å¹´è®ºæ–‡æ•°é‡çº¦ä¸ºï¼š{int(pred)} ç¯‡")

    # å¯è§†åŒ–ç»“æœ
    plt.figure(figsize=(8, 5))
    plt.bar(counts.index, counts.values, label='å®é™…')
    plt.plot(cfg['predict_years'], preds, 'ro--', label='é¢„æµ‹')
    plt.plot(X_curve, y_curve, 'g--', label='æ‹Ÿåˆæ›²çº¿')  # ç»¿è‰²è™šçº¿æ‹Ÿåˆæ›²çº¿
    plt.title(f'{conf.upper()} å¹´åº¦è®ºæ–‡æ•°é‡è¶‹åŠ¿')
    plt.xlabel('å¹´ä»½')
    plt.ylabel('è®ºæ–‡æ•°')
    plt.xticks(list(counts.index) + cfg['predict_years'])
    plt.grid(True)
    plt.legend()
    plt.tight_layout()
    plt.savefig(f'{conf}_trend_prediction.png')
    plt.show()


# ç”Ÿæˆæ¯å¹´ç»Ÿä¸€è¯äº‘ï¼ˆ2020â€“2024ï¼‰
for year in range(2020, 2025):
    titles = df[df['year'] == year]['title']
    if titles.empty:
        print(f"âš ï¸ {year} æ²¡æœ‰å¯ç”¨æ ‡é¢˜ï¼Œè·³è¿‡è¯äº‘ç”Ÿæˆã€‚")
        continue
    words = ' '.join(titles.map(clean_text)).split()
    word_freq = Counter(words)
    if not word_freq:
        print(f"âš ï¸ {year} è¯é¢‘ç»Ÿè®¡ä¸ºç©ºï¼Œæ— æ³•ç”Ÿæˆè¯äº‘ã€‚")
        continue
    wc = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(word_freq)
    plt.figure(figsize=(10, 5))
    plt.imshow(wc, interpolation='bilinear')
    plt.axis('off')
    plt.title(f'{year} å¹´ç ”ç©¶çƒ­ç‚¹è¯äº‘')
    plt.tight_layout()
    plt.savefig(f'wordcloud_{year}.png')
    plt.show()
